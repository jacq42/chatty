{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "857721d6-2965-4fae-a964-2f95b37c8799",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install required libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beca5a7-207a-4ac3-aae2-08eea92b2ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-llms-ibm==0.1.0 --user\n",
    "!pip install llama-index-embeddings-ibm==0.1.0 --user\n",
    "!pip install llama-index==0.10.65 --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a54ad5-7cf3-48f6-a633-c1be44d70b65",
   "metadata": {},
   "source": [
    "Import required libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e7ba0-3fd3-4679-9ef9-7e1f7bac4c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from llama_index.llms.ibm import WatsonxLLM\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.ibm import WatsonxEmbeddings\n",
    "from llama_index.core import VectorStoreIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549f3c62-7fdd-4c44-b520-313ccb8d8316",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "`SimpleDirectoryReader` reads data from local file to LlamaIndex. Reads all files from specified directory and processes them as text files.\n",
    "\n",
    "A `Document` acts as a container around any datas ource.\n",
    "Key attributes of a `Document`:\n",
    "* metadata: annotations that can be append to a text (name of the author, title of document)\n",
    "* relationships: reference to other documents or notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e008b879-757b-4114-b65e-466a4628d47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/pfSOEORnYBZppsnhmZ1a8A/lora-paper.pdf\"\n",
    "documents = SimpleDirectoryReader(input_files=[\"lora-paper.pdf\"]).load_data()\n",
    "# get first page of paper\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3992911-3506-4fe0-9deb-73e12b14dc5e",
   "metadata": {},
   "source": [
    "## Splitting the data\n",
    "\n",
    "Splitting into smaller chunks: dividing into nods. A `Node` is a chunk (piece of text, image, other data) and contains metadata and relationships to other nodes.\n",
    "`chunk_size` is the maximum size of each node. \n",
    "Here we use `SentenceSplitter` with a chunk_size of 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8519659-7210-453c-882d-e761d8d1af88",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = SentenceSplitter(chunk_size=500)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "len(nodes) # outputs 87\n",
    "\n",
    "node_metadata = nodes[0].get_content(metadata_mode=True)\n",
    "print(str(node_metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a75e922-4d46-4e38-81c5-6d312a0c1844",
   "metadata": {},
   "source": [
    "## Indexing the chunks\n",
    "\n",
    "An `Index` is the core foundation of RAG use-cases\n",
    "\n",
    "Embedding = converting text data into vectors\n",
    "\n",
    "Here we use the `VectorStoreIndex` from LlamaIndex: converts nodes into vector representation and stores them in a vector store\n",
    "\n",
    "After indexing we can retrieve the most relevant nodes by using the index as a retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691fe8c1-8087-46b3-b8dc-ee09b19d0ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "watsonx_embedding = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/slate-125m-english-rtrvr\",\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    project_id=\"skills-network\",\n",
    "    truncate_input_tokens=3,\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    nodes=nodes, \n",
    "    embed_model=watsonx_embedding, \n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# use index as a retriever\n",
    "base_retriever = index.as_retriever(similarity_top_k=3) # 3 for top 3 results\n",
    "source_nodes = base_retriever.retrieve(\"GPT-2\") # querying about GPT-2\n",
    "for node in source_nodes:\n",
    "    # print(node.metadata)\n",
    "    print(f\"---------------------------------------------\")\n",
    "    print(f\"Score: {node.score:.3f}\")\n",
    "    print(node.get_content())\n",
    "    print(f\"---------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87df237d-0a78-4055-a324-a7c302a0973a",
   "metadata": {},
   "source": [
    "## Querying\n",
    "\n",
    "Integrate a LLM to generate responses. LLM takes the responses from the VectorStoreIndex and generates an answer to the user input. \n",
    "First define the model\n",
    "\n",
    "Granite = decoder-only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c54de-d5da-43cc-956f-dfc77c03e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.1\n",
    "max_new_tokens = 75\n",
    "additional_params = {\n",
    "    \"decoding_method\": \"sample\",\n",
    "    \"min_new_tokens\": 1,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 1,\n",
    "}\n",
    "\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=\"ibm/granite-3-8b-instruct\",\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    project_id=\"skills-network\",\n",
    "    temperature=temperature,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    additional_params=additional_params,\n",
    ")\n",
    "\n",
    "response = watsonx_llm.complete(\"What is a Generative AI?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
