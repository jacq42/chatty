{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef5625ea-008c-41ca-9556-a1ade1c77381",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Required libs:\n",
    "* youtube-transcript-api for extracting transcripts from YouTube videos.\n",
    "* faiss-cpu for efficient similarity search.\n",
    "* langchain and langchain-community for text processing and language models.\n",
    "* ibm-watsonx-ai and langchain_ibm for integrating IBM Watson services.\n",
    "\n",
    "Install requried libs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7cda22-c44c-4d37-8fd7-1e9e2c26ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install youtube-transcript-api==0.6.2\n",
    "!pip install faiss-cpu==1.8.0\n",
    "!pip install langchain==0.2.6 | tail -n 1\n",
    "!pip install langchain-community==0.2.6 | tail -n 1\n",
    "!pip install ibm-watsonx-ai==1.0.10 | tail -n 1\n",
    "!pip install langchain_ibm==0.1.8 | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0fa3af-4482-4e56-9d79-c77080841d02",
   "metadata": {},
   "source": [
    "Import required libs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ced78-eb27-47c4-bf0c-89c375ad69cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watsonx_ai import APIClient\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from ibm_watsonx_ai.foundation_models.utils import get_embedding_model_specs\n",
    "from langchain_ibm import WatsonxEmbeddings\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import re\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09001dc9-5665-460b-b6c2-654e8396bd14",
   "metadata": {},
   "source": [
    "## Youtube transcription\n",
    "\n",
    "* manual transcripts are preferred over automatic\n",
    "* A typical YouTube URL format is: https://www.youtube.com/watch?v=VIDEO_ID\n",
    "* The transcript is represented as a list of dictionaries. Each dictionary contains:\n",
    "    * text: The spoken content.\n",
    "    * start: The starting time of the segment in seconds.\n",
    "    * duration: The duration of the segment in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6627fd3a-6a7d-48cc-a11b-00a8232c9b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_id(url):    \n",
    "    # Define a regular expression pattern to match YouTube video URLs\n",
    "    # The pattern captures 11 alphanumeric characters (plus hyphen or underscore) after '?v='\n",
    "    pattern = r'https:\\/\\/www\\.youtube\\.com\\/watch\\?v=([a-zA-Z0-9_-]{11})'\n",
    "    \n",
    "    # Search the provided URL for the pattern\n",
    "    match = re.search(pattern, url)\n",
    "    \n",
    "    # If a match is found, return the captured video ID group\n",
    "    # Otherwise, return None\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def get_transcript(url):\n",
    "    video_id = get_video_id(url)\n",
    "    # Fetches the list of available transcripts for the given YouTube video\n",
    "    srt = YouTubeTranscriptApi.list_transcripts(video_id)\n",
    "\n",
    "    transcript = \"\"\n",
    "    for i in srt:\n",
    "        # Check if the transcript is auto-generated\n",
    "        if i.is_generated:\n",
    "            # If no transcript has been set yet, use the auto-generated one\n",
    "            if len(transcript) == 0:\n",
    "                transcript = i.fetch()\n",
    "        else:\n",
    "            # If a manually created transcript is found, use it (overrides auto-generated)\n",
    "            transcript = i.fetch()\n",
    "\n",
    "    return transcript\n",
    "\n",
    "# Retrieve the transcript for the specified YouTube video URL\n",
    "transcript = get_transcript(\"https://www.youtube.com/watch?v=kEOCrtkLvEo&t=24s\")\n",
    "\n",
    "# Display the first 10 entries of the transcript\n",
    "# Each entry is a dictionary containing 'text', 'start', and 'duration'\n",
    "transcript[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b8040d-5323-4d12-87d1-4c6836b221aa",
   "metadata": {},
   "source": [
    "Process the fetched transcript into readable content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8f3cc1-b0ee-4d70-bd53-07c7cdf826c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(transcript):\n",
    "    # Initialize an empty string to accumulate processed text\n",
    "    txt = \"\"\n",
    "\n",
    "    # Iterate over each segment in the transcript\n",
    "    for i in transcript:\n",
    "        try:\n",
    "            # Format the text and start time, then add to the accumulated string\n",
    "            txt += f\"Text: {i['text']} Start: {i['start']}\\n\"\n",
    "        except:\n",
    "            # If an error occurs (e.g., missing keys), skip the entry\n",
    "            pass\n",
    "\n",
    "    # Return the processed text\n",
    "    return txt\n",
    "\n",
    "processed_transcript = process(transcript)\n",
    "processed_transcript[:100] # Display the first 100 characters of the processed transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9784a0f9-04d3-4070-99e6-2c3bca79023a",
   "metadata": {},
   "source": [
    "## Chunking the transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b175340-3944-43b7-bdc1-0a9fc70b05c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,  # Maximum chunk size of 200 characters\n",
    "    chunk_overlap=20  # Overlap of 20 characters between chunks\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(processed_transcript)\n",
    "chunks[:10]  # Display the first 10 chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f2d770-b19a-4570-b5ed-0d68ea4c3124",
   "metadata": {},
   "source": [
    "Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2939901-94a5-4181-98b1-a13e8af1abfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model ID for the Granite 8B Instruct Generation 3 model\n",
    "model_id = \"ibm/granite-3-8b-instruct\"\n",
    "\n",
    "# Set up the credentials needed to access the IBM Watson services\n",
    "credentials = Credentials(\n",
    "    url = \"https://us-south.ml.cloud.ibm.com\",\n",
    ")\n",
    "\n",
    "# Initialize the API client with the given credentials\n",
    "client = APIClient(credentials)\n",
    "\n",
    "# Define the project ID for organizing tasks within IBM Watson services\n",
    "project_id = \"skills-network\"\n",
    "\n",
    "parameters = {\n",
    "    # Specifies the decoding method as greedy decoding\n",
    "    # This means the model always chooses the most probable next token\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
    "    \n",
    "    # Sets the minimum number of new tokens to generate to 1\n",
    "    # The model will always produce at least this many tokens\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    \n",
    "    # Sets the maximum number of new tokens to generate to 500\n",
    "    # The model will stop generating after reaching this limit\n",
    "    GenParams.MAX_NEW_TOKENS: 500,\n",
    "    \n",
    "    # Defines sequences that will cause the generation to stop\n",
    "    # In this case, generation will stop when encountering two consecutive newlines\n",
    "    GenParams.STOP_SEQUENCES: [\"\\n\\n\"],\n",
    "}\n",
    "\n",
    "watsonx_granite = WatsonxLLM(\n",
    "    # Specifies the ID of the model to be used\n",
    "    # This is likely an enum or constant value defining a specific model\n",
    "    model_id=model_id,\n",
    "    \n",
    "    # The URL endpoint for the Watson service\n",
    "    # This is retrieved from a credentials dictionary\n",
    "    url=credentials.get(\"url\"),\n",
    "    \n",
    "    # The ID of the project in which this LLM instance will operate\n",
    "    # This helps in organizing and managing different LLM instances\n",
    "    project_id=project_id,\n",
    "    \n",
    "    # A dictionary of parameters that configure the behavior of the LLM\n",
    "    # This includes settings like decoding method, token limits, and stop sequences\n",
    "    params=parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2890a57-defe-4d16-b0c6-0f86f84d651d",
   "metadata": {},
   "source": [
    "Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae9338-fa23-4eea-9f82-ae531d005aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch specifications for available embedding models from the Watson service\n",
    "get_embedding_model_specs(credentials.get('url'))\n",
    "\n",
    "# Part 1: Create Embedding Model\n",
    "# Set up the WatsonxEmbeddings object\n",
    "embeddings = WatsonxEmbeddings(\n",
    "    # Specifies the ID of the embedding model to be used\n",
    "    # In this case, it's using the IBM SLATE 30M English model\n",
    "    model_id=EmbeddingTypes.IBM_SLATE_30M_ENG.value,\n",
    "    \n",
    "    # The URL endpoint for the Watson service\n",
    "    # This is retrieved from the credentials dictionary\n",
    "    url=credentials[\"url\"],\n",
    "    \n",
    "    # The ID of the project in which this embedding model will operate\n",
    "    # This helps in organizing and managing different model instances\n",
    "    project_id=project_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fec7ed-ef19-4438-b5ae-37d21ac8842a",
   "metadata": {},
   "source": [
    "## Implementing FAISS\n",
    "\n",
    "FAISS = Facebook AI Similarity Search\n",
    "Converting transcript chunks in embeddings and store them in FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01d5fe2-89df-4acd-8273-80c60c018f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FAISS index from the text chunks using the specified embeddings\n",
    "faiss_index = FAISS.from_texts(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50404f66-8ab6-4b69-beea-0ec61f8f3126",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "In this example, we asked the question \"Which company were they talking about?\" The similarity search algorithm successfully retrieved relevant chunks of text from the transcript.\n",
    "\n",
    "1. **Identifying the Company**: The first result directly answers our query. It mentions \"IBM\" twice:\n",
    "   - \"she's recently placed in IBM\"\n",
    "   - \"interview experience of IBM\"\n",
    "   \n",
    "   This clearly indicates that the company being discussed is IBM.\n",
    "\n",
    "2. **How We Fetched It**:\n",
    "   - The FAISS index we created earlier contains vector representations (embeddings) of all the text chunks from the transcript.\n",
    "   - When we input our query, it's also converted into a vector using the same embedding model.\n",
    "   - The `similarity_search` function then finds the chunks whose vector representations are most similar to our query vector.\n",
    "   - We requested the top 3 most similar chunks (`k=3`), which are returned and printed.\n",
    "\n",
    "3. **Context**: The other results provide additional context about the interview process, mentioning details like:\n",
    "   - There were multiple interviewers, including a senior software developer.\n",
    "   - There was a technical round in the interview process.\n",
    "\n",
    "This demonstrates the power of semantic search: even though our query didn't use the exact words found in the transcript, the system was able to understand the intent of the question and retrieve relevant information, successfully identifying IBM as the company being discussed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761c27b2-c52c-4b81-b5d8-4570bc397b42",
   "metadata": {},
   "source": [
    "## Summarizing with LangChain\n",
    "\n",
    "Define a prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e5a822-9fef-448f-a5f2-0e643d79eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template for summarizing the transcript\n",
    "prompt = PromptTemplate(\n",
    "    # Specify the input variables that will be used in the template\n",
    "    input_variables=[\"transcript\"],\n",
    "    \n",
    "    # Define the actual template string\n",
    "    template=\"\"\"\n",
    "Summarize the following YouTube video transcript in terms of paragraph:\n",
    "\n",
    "{transcript}\n",
    "\n",
    "Your summary should have concise summary in terms of paragraph. Ignore any timestamps.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Instantiate LLMChain with the refined prompt and LLM\n",
    "summarise_chain = LLMChain(\n",
    "    llm=watsonx_granite,  # The language model to be used (in this case, watsonx_granite)\n",
    "    prompt=prompt,        # The PromptTemplate we defined earlier\n",
    "    verbose=True          # Enable verbose mode for detailed output\n",
    ")\n",
    "\n",
    "# Return the instantiated LLMChain object\n",
    "summarise_chain\n",
    "\n",
    "# Pass the processed transcript to the LLMChain for summarization\n",
    "summary = summarise_chain.predict(transcript=processed_transcript)\n",
    "\n",
    "# Print the generated summary\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906d7d79-9e85-46ed-b651-1f3fbb7f4a05",
   "metadata": {},
   "source": [
    "## Retrieve relevant context and generate answers\n",
    "\n",
    "### Explanation \n",
    "\n",
    "1. **User query**: \n",
    "   - A user asks a question about a specific YouTube video or a topic covered in multiple videos.\n",
    "\n",
    "2. **Query embedding**: \n",
    "   - The user's question is converted into a vector embedding using the same model used for processing video transcripts (e.g., IBM SLATE-30M).\n",
    "\n",
    "3. **Similarity search**: \n",
    "   - The query embedding is compared to the embeddings of transcript chunks stored in your vector database (FAISS index). \n",
    "   - These transcript chunks represent segments of YouTube video.\n",
    "\n",
    "4. **Retrieval**: \n",
    "   - The system retrieves the most similar transcript chunks based on the similarity scores. \n",
    "   - These chunks are likely to contain information relevant to the user's question.\n",
    "\n",
    "5. **Context assembly**: \n",
    "   - The retrieved transcript chunks are combined to form the relevant context. \n",
    "   - This context might include portions of transcripts from the youtube video, depending on the query and available content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e7ba3-5f9c-4b1c-874b-b96c50f3915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query):\n",
    "    # Perform a similarity search on the FAISS index\n",
    "    relevant_context = faiss_index.similarity_search(query, k=7)\n",
    "    return relevant_context\n",
    "\n",
    "# Define the template for question answering\n",
    "qa_template = \"\"\"\n",
    "You are an expert assistant providing detailed answers based on the following video content.\n",
    "\n",
    "Relevant Video Context: {context}\n",
    "\n",
    "Based on the above context, please answer the following question:\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Create the PromptTemplate object\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],  # Variables to be filled dynamically\n",
    "    template=qa_template\n",
    ")\n",
    "\n",
    "# Create a Question-Answering LLM Chain\n",
    "QAChain = LLMChain(\n",
    "    llm=watsonx_granite,     # The language model to be used (watsonx_granite in this case)\n",
    "    prompt=prompt_template,  # The PromptTemplate we defined earlier for structuring the input\n",
    "    verbose=True             # Enable verbose mode for detailed output\n",
    ")\n",
    "\n",
    "QAChain\n",
    "\n",
    "def generate_answer(question):\n",
    "    # Retrieve relevant context based on the question\n",
    "    relevant_context = retrieve(question)\n",
    "    \n",
    "    # Use the QAChain to generate an answer based on the context and question\n",
    "    answer = QAChain.predict(context=relevant_context, question=question)\n",
    "    \n",
    "    # Return the generated answer\n",
    "    return answer\n",
    "\n",
    "# Input question (We can ask: Is there a coding round?)\n",
    "question = input(\"Enter your question: \")\n",
    "\n",
    "# Generate the answer\n",
    "answer = generate_answer(question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
