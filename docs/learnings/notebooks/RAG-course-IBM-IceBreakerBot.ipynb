{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d44baff-2387-4590-8acb-a7610678ce39",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Required libs:\n",
    "* ibm-watsonx-ai for accessing the watsonx Granite language model.\n",
    "* llama-index-llms-ibm for communicating with watsonx.ai models using the LlamaIndex and watsonx.ai's LLMs API.\n",
    "* llama-index-embeddings-ibm for using watsonx.ai's embedding models.\n",
    "* llama-index for using LlamaIndex framework relevant features.\n",
    "\n",
    "Install required libs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13767af-1f59-4279-99c6-45ed728866e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install ibm-watsonx-ai==1.1.2\n",
    "!pip install --user llama-index==0.11.8\n",
    "!pip install llama-index-core==0.11.8\n",
    "!pip install llama-index-llms-ibm==0.2.0\n",
    "!pip install llama-index-embeddings-ibm==0.2.0\n",
    "!pip install llama-index-readers-web==0.2.2\n",
    "!pip install llama-hub==0.0.79.post1\n",
    "!pip install requests==2.32.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb1a04b-4276-4825-aa84-26f83b7d5cf7",
   "metadata": {},
   "source": [
    "Import required libs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66ec922-50c5-4102-9461-347664b48f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "logging.getLogger(\"ibm_watsonx_ai\").setLevel(logging.ERROR)\n",
    "\n",
    "# IBM Watsonx API Client and Credentials handling\n",
    "from ibm_watsonx_ai import APIClient, Credentials\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "# Core components for handling documents, embeddings, and indices\n",
    "from llama_index.core import Document, VectorStoreIndex, PromptTemplate, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# LlamaIndex IBM-specific components for LLMs and embeddings\n",
    "from llama_index.llms.ibm import WatsonxLLM\n",
    "from llama_index.embeddings.ibm import WatsonxEmbeddings\n",
    "\n",
    "# For displaying rich content in Jupyter notebooks (Markdown, etc.)\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Disable warnings for a cleaner notebook or console experience\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b4189-c6e4-4b19-b870-c58af1575627",
   "metadata": {},
   "source": [
    "## Extracting LinkedIn profile data\n",
    "\n",
    "[ProxyCurl](https://nubela.co/proxycurl/) is a robust API that allows developers to extract information from various websites, including social media platforms like LinkedIn\n",
    "There is no free version for ProxyCurl. Alternatively use mocked data.\n",
    "\n",
    "While LlamaIndex provides a built-in Web Page Reader for reading websites, it cannot extract LinkedIn data. To overcome this, we utilize the ProxyCurl API, which provides a reliable and efficient way to extract LinkedIn profiles data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1465ddd4-9c7a-41b4-beca-b1bffef86872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with ProxyCurl\n",
    "PROXYCURL_API_KEY = \"# Replace with your API Key\" \n",
    "\n",
    "# with mocked data\n",
    "def extract_linkedin_profile(linkedin_profile_url: str, PROXYCURL_API_KEY: str = None, mock: bool = False) -> dict:\n",
    "    \"\"\"Extract LinkedIn profile data using Proxycurl API or loads a premade JSON file if mock is True.\"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if mock:\n",
    "        print(\"Using mock data from a premade JSON file...\")\n",
    "        linkedin_profile_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ZRe59Y_NJyn3hZgnF1iFYA/linkedin-profile-data.json\"\n",
    "        response = requests.get(linkedin_profile_url, timeout=30)\n",
    "    else:\n",
    "        # Ensure API key is provided when mock is False\n",
    "        if not PROXYCURL_API_KEY:\n",
    "            raise ValueError(\"PROXYCURL_API_KEY is required when mock is set to False.\")\n",
    "        \n",
    "        print(\"Starting to extract the LinkedIn profile...\")\n",
    "\n",
    "        # Set up the API endpoint and headers\n",
    "        api_endpoint = \"https://nubela.co/proxycurl/api/v2/linkedin\"\n",
    "        headers = {\n",
    "            \"Authorization\": PROXYCURL_API_KEY\n",
    "        }\n",
    "\n",
    "        # Prepare parameters for the request\n",
    "        params = {\n",
    "            \"url\": linkedin_profile_url,\n",
    "            \"fallback_to_cache\": \"on-error\",\n",
    "            \"use_cache\": \"if-present\",\n",
    "            \"skills\": \"include\",\n",
    "            \"inferred_salary\": \"include\",\n",
    "            \"personal_email\": \"include\",\n",
    "            \"personal_contact_number\": \"include\"\n",
    "        }\n",
    "\n",
    "        print(f\"Sending API request to Proxycurl at {time.time() - start_time:.2f} seconds...\")\n",
    "\n",
    "        # Send API request\n",
    "        response = requests.get(api_endpoint, headers=headers, params=params, timeout=10)\n",
    "    \n",
    "    print(f\"Received response at {time.time() - start_time:.2f} seconds...\")\n",
    "\n",
    "    # Check if response is successful\n",
    "    if response.status_code == 200:\n",
    "        # Clean the data, remove empty values and unwanted fields\n",
    "        data = response.json()\n",
    "        data = {\n",
    "            k: v\n",
    "            for k, v in data.items()\n",
    "            if v not in ([], \"\", None) and k not in [\"people_also_viewed\", \"certifications\"]\n",
    "        }\n",
    "\n",
    "        if data.get(\"groups\"):\n",
    "            for group_dict in data.get(\"groups\"):\n",
    "                group_dict.pop(\"profile_pic_url\")\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "        return {}\n",
    "\n",
    "profile_url = \"https://www.linkedin.com/in/leonkatsnelson/\"\n",
    "\n",
    "# with ProxyCurl\n",
    "profile_data = extract_linkedin_profile(linkedin_profile_url=profile_url, PROXYCURL_API_KEY=PROXYCURL_API_KEY, mock=False)\n",
    "# with mocked data\n",
    "profile_data = extract_linkedin_profile(linkedin_profile_url=\"dummy_url\", mock=True)\n",
    "\n",
    "profile_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1852ff8-517d-4347-b1e4-6df5debfeb31",
   "metadata": {},
   "source": [
    "Splitting into nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d648c5-8a9e-4289-9e1c-286942d1a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_profile_data(profile_data):\n",
    "    \"\"\"Splits the LinkedIn profile JSON data into nodes.\"\"\"\n",
    "    try:\n",
    "        # The extracted LinkedIn profile data is returned in JSON format. To work with this data more easily, \n",
    "        # we first convert it into a text string using the json.dumps() function. \n",
    "        # This transformation allows us to manipulate the data in subsequent steps, \n",
    "        # such as splitting it for further processing.\n",
    "        profile_json = json.dumps(profile_data)\n",
    "\n",
    "        # Once the JSON string is created, it is wrapped inside a `Document` object. \n",
    "        # This step is necessary because the `Document` format is required for the splitting \n",
    "        # and processing steps that follow. The `Document` serves as a container for the profile data, \n",
    "        # enabling structured handling of the information.\n",
    "        document = Document(text=profile_json)\n",
    "\n",
    "        # To break down the document into smaller parts, we utilize the `SentenceSplitter` class. \n",
    "        # This tool splits the document into manageable chunks, called `nodes`. \n",
    "        # The parameter `chunk_size=500` is used, meaning each node will contain approximately 500 characters. \n",
    "        # This ensures that each chunk is small enough for efficient processing while maintaining coherence for the model to understand.\n",
    "        splitter = SentenceSplitter(chunk_size=500)\n",
    "\n",
    "        # Once the document is split, the function returns a list of nodes. \n",
    "        # Each node represents a portion of the original LinkedIn profile data, \n",
    "        # and these chunks will later be stored in a vector database. \n",
    "        # This step is crucial for enabling efficient indexing and retrieval in future operations.\n",
    "        nodes = splitter.get_nodes_from_documents([document])\n",
    "        return nodes\n",
    "        \n",
    "    # The entire function is wrapped in a `try-except` block to manage potential errors. \n",
    "    # If something goes wrong during the process, the function catches the error, \n",
    "    # prints an error message for debugging, and returns an empty list. \n",
    "    # This helps ensure the program remains stable, even when issues arise.\n",
    "    except Exception as e:\n",
    "        print(f\"Error in split_profile_data: {e}\")\n",
    "        return []\n",
    "\n",
    "nodes = split_profile_data(profile_data)\n",
    "\n",
    "print(f\"Number of nodes created: {len(nodes)}\")\n",
    "\n",
    "# Print the first few nodes for inspection\n",
    "for i, node in enumerate(nodes[:5]):\n",
    "    print(f\"\\nNode {i+1}:\")\n",
    "    print(node.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2ae535-d8ee-4833-a4ed-f05fb23229e1",
   "metadata": {},
   "source": [
    "Indexing and Storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283cbb90-cc1d-4e12-a587-06e99640c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_watsonx_embedding():\n",
    "    \"\"\"Creates an IBM Watsonx Embedding model for vector representation.\"\"\"\n",
    "    watsonx_embedding = WatsonxEmbeddings(\n",
    "        model_id=\"ibm/slate-125m-english-rtrvr\",\n",
    "        url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "        project_id=\"skills-network\",\n",
    "        truncate_input_tokens=3,\n",
    "    )\n",
    "    return watsonx_embedding\n",
    "\n",
    "def vector_database(nodes):\n",
    "    \"\"\"Stores the document chunks (nodes) in a vector database.\"\"\"\n",
    "    try:\n",
    "        # We first call the `create_watsonx_embedding()` function to \n",
    "        # get the IBM watsonx embedding model, which will embed our nodes.\n",
    "        embedding_model = create_watsonx_embedding()\n",
    "\n",
    "        # The VectorStoreIndex class is used to embed the nodes and \n",
    "        # store the resulting vector representations in a vector database.\n",
    "        index = VectorStoreIndex(\n",
    "            nodes=nodes, # These are the chunks of text (or nodes) that were created in the previous splitting step.\n",
    "            embed_model=embedding_model, # The embedding model used to convert text into vectors.\n",
    "            show_progress=False # This hides the progress bar during the embedding process, \n",
    "                                # but you can set this to True if you want to track the embedding progress.\n",
    "        )\n",
    "        return index\n",
    "    # The entire indexing process is wrapped in a `try-except` block to catch \n",
    "    # and display any errors that may occur during the embedding or storing process.\n",
    "    except Exception as e:\n",
    "        print(f\"Error in vector_database: {e}\")\n",
    "        return None\n",
    "\n",
    "vectordb_index = vector_database(nodes)\n",
    "\n",
    "# test by indexing\n",
    "if vectordb_index:\n",
    "    print(\"Vector database created successfully.\")\n",
    "else:\n",
    "    print(\"Failed to create vector database.\")\n",
    "\n",
    "# Inspect embeddings\n",
    "vector_store = vectordb_index._storage_context.vector_store\n",
    "node_ids = list(vectordb_index.index_struct.nodes_dict.keys())\n",
    "missing_embeddings = False\n",
    "\n",
    "for node_id in node_ids:\n",
    "    embedding = vector_store.get(node_id)\n",
    "    if embedding is None:\n",
    "        print(f\"Node ID {node_id} has a None embedding.\")\n",
    "        missing_embeddings = True\n",
    "    else:\n",
    "        print(f\"Node ID {node_id} has a valid embedding.\")\n",
    "\n",
    "if missing_embeddings:\n",
    "    print(\"Some node embeddings are missing. Please check the embedding generation step.\")\n",
    "else:\n",
    "    print(\"All node embeddings are valid.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa456a2f-ddca-46b6-a983-e902fe7b7440",
   "metadata": {},
   "source": [
    "Quering with prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16295da3-2837-46ed-a072-e1ef3bd6850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_facts_template = \"\"\"\n",
    "You are an AI assistant that provides detailed answers based on the provided context.\n",
    "\n",
    "Context information is below:\n",
    "\n",
    "{context_str}\n",
    "\n",
    "Based on the context provided, list 3 interesting facts about this person's career or education.\n",
    "\n",
    "Answer in detail, using only the information provided in the context.\n",
    "\"\"\"\n",
    "initial_facts_prompt = PromptTemplate(template=initial_facts_template)\n",
    "\n",
    "\n",
    "user_question_template = \"\"\"\n",
    "You are an AI assistant that provides detailed answers to questions based on the provided context.\n",
    "\n",
    "Context information is below:\n",
    "\n",
    "{context_str}\n",
    "\n",
    "Question: {query_str}\n",
    "\n",
    "Answer in full details, using only the information provided in the context.If the answer is not available in the context, say \"I don't know. The information is not available on the LinkedIn page.\"\n",
    "\"\"\"\n",
    "user_question_prompt = PromptTemplate(template=user_question_template)\n",
    "\n",
    "def generate_initial_facts(index):\n",
    "    \"\"\"Generates 3 interesting facts about the person's career or education.\"\"\"\n",
    "\n",
    "    # Set the temperature for the model's response generation (controls creativity of the response).\n",
    "    temperature = 0.0\n",
    "    # Set the maximum number of new tokens (words) to generate in the response.\n",
    "    max_new_tokens = 500\n",
    "    additional_params = {\n",
    "        \"decoding_method\": \"sample\",  # Sample from the probability distribution of tokens (instead of greedy decoding).\n",
    "        \"min_new_tokens\": 1,          # Minimum number of tokens to generate.\n",
    "        \"top_k\": 50,                  # Consider the top 50 most likely tokens at each step in the generation process.\n",
    "        \"top_p\": 1,                   # Use nucleus sampling with a probability cutoff at 1 (i.e., consider all tokens).\n",
    "    }\n",
    "\n",
    "    # Initialize the WatsonxLLM instance for the ibm/granite-3-8b-instruct model\n",
    "    watsonx_llm = WatsonxLLM(\n",
    "        model_id=\"ibm/granite-3-8b-instruct\",\n",
    "        url=\"https://us-south.ml.cloud.ibm.com\", \n",
    "        project_id=\"skills-network\",              \n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        additional_params=additional_params,\n",
    "    )\n",
    "    \n",
    "    # Create a query engine using the initial facts prompt\n",
    "    query_engine = index.as_query_engine(\n",
    "        streaming=False,                        # Disable streaming, wait for full response at once.\n",
    "        similarity_top_k=5,                     # Use top 5 similar items from the index for query.\n",
    "        llm=watsonx_llm,                        # Pass the Watsonx LLM with the IBM Granite model.\n",
    "        text_qa_template=initial_facts_prompt    # Use a predefined prompt template to structure the LLM's output.\n",
    "    )\n",
    "    \n",
    "    # Define a query that asks for 3 interesting facts about a person's career or education.\n",
    "    query = \"Provide three interesting facts about this person's career or education.\"\n",
    "    \n",
    "    # Execute the query using the query engine.\n",
    "    response = query_engine.query(query)\n",
    "    \n",
    "    # Extract the actual generated facts from the response object.\n",
    "    facts = response.response\n",
    "\n",
    "    # Return the generated facts.\n",
    "    return facts\n",
    "\n",
    "from llama_index.llms.ibm import WatsonxLLM\n",
    "\n",
    "def answer_user_query(index, user_query):\n",
    "    \"\"\"Answers the user's question using the vector database and the LLM.\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Set the temperature for controlling the randomness of the LLM's response.\n",
    "        temperature = 0.0\n",
    "        # Limit the number of new tokens generated in the response to 250.\n",
    "        max_new_tokens = 250\n",
    "        additional_params = {\n",
    "            \"decoding_method\": \"greedy\",  # Greedy decoding for deterministic and predictable response.\n",
    "            \"min_new_tokens\": 1,\n",
    "            \"top_k\": 50,\n",
    "            \"top_p\": 1,\n",
    "        }\n",
    "        \n",
    "        # Initialize the WatsonxLLM instance for the ibm/granite-3-8b-instruct model\n",
    "        watsonx_llm = WatsonxLLM(\n",
    "            model_id=\"ibm/granite-3-8b-instruct\",\n",
    "            url=\"https://us-south.ml.cloud.ibm.com\", \n",
    "            project_id=\"skills-network\",              \n",
    "            temperature=temperature,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            additional_params=additional_params,\n",
    "        )\n",
    "\n",
    "        # Retrieve relevant nodes (chunks of data) from the index based on the user query.\n",
    "        base_retriever = index.as_retriever(similarity_top_k=5)  # Fetch top 5 most relevant items from the index.\n",
    "        source_nodes = base_retriever.retrieve(user_query)       # Retrieve relevant data chunks based on the query.\n",
    "\n",
    "        # Build a context string by joining the text from each retrieved node.\n",
    "        context_str = \"\\n\\n\".join([node.node.get_text() for node in source_nodes])\n",
    "        \n",
    "        # Create a query engine, specifying how the LLM should answer questions based on user input and the context.\n",
    "        query_engine = index.as_query_engine(\n",
    "            streaming=False,                        # Disable streaming, get the complete response all at once.\n",
    "            similarity_top_k=5,                     # Use the top 5 similar items from the index for the query.\n",
    "            llm=watsonx_llm,                        # Use the Watsonx LLM with the IBM Granite model.\n",
    "            text_qa_template=user_question_prompt    # Provide a template to guide the LLM in forming the response.\n",
    "        )\n",
    "        \n",
    "        # Execute the query with the user's question and return the LLM's answer.\n",
    "        answer = query_engine.query(user_query)\n",
    "        return answer\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Handle exceptions gracefully and log the error.\n",
    "        print(f\"Error in answer_user_query: {e}\")\n",
    "        return \"Failed to get an answer.\"\n",
    "\n",
    "# test without vector database\n",
    "initial_facts = generate_initial_facts(vectordb_index)\n",
    "print(\"\\nHere are 3 interesting facts about this person:\")\n",
    "print(initial_facts)\n",
    "\n",
    "user_query = \"What is this person's current job title?\"\n",
    "response = answer_user_query(vectordb_index, user_query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b451314d-d95b-4182-b53b-f10b81183746",
   "metadata": {},
   "source": [
    "## Building a chatbot interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eeff3a-4afb-4228-ba38-dc4b7aac602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_interface(index):\n",
    "    \"\"\"Provides a simple chatbot interface for user interaction.\"\"\"\n",
    "    print(\"\\nYou can now ask more in-depth questions about this person. Type 'exit', 'quit' or 'bye' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        user_query = input(\"You: \")\n",
    "        if user_query.lower() in ['exit', 'quit', 'bye']:\n",
    "            print(\"Bot: Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        print(\"Bot is typing...\", end='')\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(1)  # Simulate typing delay\n",
    "        print('\\r', end='')\n",
    "        \n",
    "        response = answer_user_query(index, user_query)\n",
    "        print(f\"Bot: {response.response.strip()}\\n\")\n",
    "\n",
    "def process_linkedin(linkedin_url, PROXYCURL_API_Key=None, mock=False):\n",
    "    \"\"\"\n",
    "    Processes a LinkedIn URL, extracts data from the profile, and interacts with the user.\n",
    "\n",
    "    Parameters:\n",
    "    - linkedin_url (str): The LinkedIn profile URL to extract or load mock data from.\n",
    "    - PROXYCURL_API_Key (str, optional): Proxycurl API key. Required if mock is False.\n",
    "    - mock (bool, optional): If True, loads mock data from a premade JSON file instead of using the API.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract the profile (with or without the API depending on the mock flag)\n",
    "        profile_data = extract_linkedin_profile(linkedin_url, PROXYCURL_API_Key, mock=mock)\n",
    "        \n",
    "        if not profile_data:\n",
    "            print(\"Failed to retrieve profile data.\")\n",
    "            return\n",
    "\n",
    "        # Split the data into nodes\n",
    "        nodes = split_profile_data(profile_data)\n",
    "        \n",
    "        # Store in vector database\n",
    "        vectordb_index = vector_database(nodes)\n",
    "        \n",
    "        # Generate and display the initial facts\n",
    "        initial_facts = generate_initial_facts(vectordb_index)\n",
    "        \n",
    "        print(\"\\nHere are 3 interesting facts about this person:\")\n",
    "        print(initial_facts)\n",
    "        \n",
    "        # Start the chatbot interface\n",
    "        chatbot_interface(vectordb_index)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "\n",
    "chatbot_interface(vectordb_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
